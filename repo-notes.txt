# Levels of abstraction


Highest:
- hydra manages work_dir, data_dir, debug, output paths
- hydra manages configs for each experiment. Each experiment must specify
    1. trainer settings  # what hardware is used during training, num epochs, etc. basically pytorch lighting Trainers settings (cant be changed)
    2. model
    3. optimizer
    4. datamodule
    5. callbacks
    6. logger
- src/train.py does the following:
    1. create pytorch lightning trainer
    2. create datamodule as specified in the config
    3. call trainer.fit
    4. call trainer.test
    5. returns metric score in case of hyperparam optimization


# TODO
Datamodules
1. Make a Dataset for the truncated Gaussian mixtures
2. Remove defaults in every config. Refactor config.yaml and experiments/*.yaml
so that every experiment is given by a different yaml file

# Experiments
- waterbirds reweighted (done)
- waterbirds polyloss (done)
- cifar binary reweighted
- cifar binary polyloss
- celeba reweighted (done)
- celeba polyloss (done)


# Tuning procedure:
## Regular batch training
TODO: Need to all use the same architecture
-All train for 200 epochs
-All use only SGD + momentum (no weight decay)
1. For ce loss models, fix the batch size (64), grid search the best LR
2. For poly loss model, use the same batch size as reweighted model (64), grid search the best LR
## Large batch training
3. For ce loss model, fix the batch size (512), grid search the best LR
4. For poly loss model, use the same batch size (512), grid search the best LR

Do the above for waterbirds, celeba, binary cifar. (All binary classification)
This gives a total of LR_grid x 2 x 2 x 3 = LR_grid x 12 models to train.


Training
1. Train recall, precision, accuracy of all classes
2. Maybe I should fix the total number of samples and then divide them that way?
3. Terminate on nan!!!

Testing
1. Validate with different classes than train classes

Logging


Done.
Don't save weights to wandb 
Log the seed for the experiments
Make wandb log the logdir for each experiment.
Right now, only settings passed to pytorch lightning are saved in wandb config, i.e. model, optimizer, datamodule, callbacks, trainer. Want to also save seed, work_dir, data_dir, debug, seed, etc.
