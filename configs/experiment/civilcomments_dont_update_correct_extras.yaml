# @package _global_
# settings from https://github.com/p-lambda/wilds/blob/e95bba8408aff524b48b96a4e7648df72773ad60/examples/configs/datasets.py#L85

defaults:
    - /datamodule: civilcomments_datamodule.yaml
    - /model: imbalanced_classifier_model.yaml
    - /optimizer: adamw.yaml # TODO: remove optimization bells and whistles
    - /architecture: distilbert_net.yaml                 
    - /loss_fn: cross_entropy.yaml
    - /lr_scheduler: linear.yaml  # TODO: remove optimization bells and whistles

logger:
    wandb:
        tags: ["civilcomments", "undersampled-with-extras"]

load_wandb_run: ["kealexanderwang", "importance-reweighing", "civilcomments-undersampled-546", "last.ckpt"]

trainer:
    min_epochs: 1
    max_epochs: 5
    gradient_clip_val: 1.0

datamodule:
    wrapper_type: undersampled_with_extras

    tokenizer:
        pretrained_model_name_or_path: ${architecture.model_name}

    batch_size: 16
    num_workers: 4
    pin_memory: True
    flatten_input: False

optimizer:
    lr: 0.00001
    weight_decay: 0.01
    # TODO: remove optimization bells and whistles

model:
    dont_update_correct_extras: True
