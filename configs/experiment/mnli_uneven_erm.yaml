# @package _global_
# settings from https://github.com/p-lambda/wilds/blob/e95bba8408aff524b48b96a4e7648df72773ad60/examples/configs/datasets.py#L85

defaults:
    - /datamodule: mnli_datamodule.yaml
    - /model: imbalanced_classifier_model.yaml
    - /optimizer: adamw.yaml
    - /architecture: distilbert_net.yaml                 
    - /loss_fn: cross_entropy.yaml
    - /lr_scheduler: linear.yaml

logger:
    wandb:
        tags: ["mnli", "erm"]

callbacks:
    group_train_accuracy_monitor:
        _target_: src.pl_callbacks.metrics_callbacks.GroupTrainAccuracyMonitor
    group_val_accuracy_monitor:
        _target_: src.pl_callbacks.metrics_callbacks.GroupValReweightedAccuracyMonitor

trainer:
    min_epochs: 1
    max_epochs: 5
    gradient_clip_val: 1.0

#Train class counts: Counter({2: 130903, 1: 130900, 0: 130899})
#Train group counts: Counter({3: 83348, 1: 77350, 4: 77350, 0: 77348, 2: 77306})
#Val class counts: Counter({0: 3479, 2: 3213, 1: 3123})
#Val group counts: Counter({4: 1976, 0: 1973, 3: 1966, 2: 1955, 1: 1945})
#Total in train should be 392702

datamodule:
    tokenizer:
        pretrained_model_name_or_path: ${architecture.model_name}

    train_frac: 1.0
    new_group_sizes: [5000, 4000, 3000, 2000, 1000]

    batch_size: 16
    num_workers: 4
    pin_memory: True
    flatten_input: False

optimizer:
    lr: 0.00001
    weight_decay: 0.01

