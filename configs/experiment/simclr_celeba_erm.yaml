# @package _global_

# to execute this experiment run:
# python run.py +experiment=simclr_celeba_erm

defaults:
    - /datamodule: celeba_datamodule.yaml
    - /model: imbalanced_classifier_model.yaml
    - /optimizer: sgd.yaml                    
    - /architecture: simclr_net.yaml                 
    - /loss_fn: cross_entropy.yaml

logger:
    wandb:
        tags: ["binary", "celeba", "erm", "simclr"]

trainer:
    min_epochs: 1
    max_epochs: 50

datamodule:
    resolution: [224, 224]
    batch_size: 64
    num_workers: 4
    pin_memory: True
    flatten_input: False

model:
    freeze_features: False

optimizer:
    lr: 0.0001
    momentum: 0.9
    weight_decay: 0.0001

architecture:
    width_mult: 1

