# @package _global_

# based on https://github.com/p-lambda/wilds/blob/e95bba8408aff524b48b96a4e7648df72773ad60/examples/configs/datasets.py#L108
# TODO: implement n_groups_per_batch? https://github.com/p-lambda/wilds/blob/e95bba8408aff524b48b96a4e7648df72773ad60/examples/configs/datasets.py#L130

defaults:
    - /datamodule: fmow_datamodule.yaml
    - /model: imbalanced_classifier_model.yaml
    - /optimizer: adam.yaml
    - /architecture: dense_net.yaml
    - /loss_fn: cross_entropy.yaml
    - /lr_scheduler: step

logger:
    wandb:
        tags: ["fmow", "undersampled"]

trainer:
    min_epochs: 1
    max_epochs: 50

datamodule:
    wrapper_type: undersampled

    groupby_fields: ["year"]

    batch_size: 64
    num_workers: 4
    pin_memory: True
    flatten_input: False

optimizer:
    lr: 0.0001
    weight_decay: 0.

lr_scheduler:
    gamma: 0.96
    step_size: 1

architecture:
    arch: densenet121
